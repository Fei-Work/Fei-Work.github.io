---
title: "dl-r-02卷积深度视觉"

categories:
  - blogs
tags:
  - visuall
  - dl
lang: zh
header: 
  teaser: "./assets/images/dl02/image-20250622152052703.png"
---

在前文的讨论中，我们从线性回归出发，一步步延展到全连接的多层网络，也使用了一个多层网络去进行图像处理。但是值得注意的是，事实上，我们是仅仅将这些图像数据展平成了一维的向量，很显然这忽略了图像的空间特征【我们都知道图像的数据存储模式是一个多维的矩阵，嗯，是的都知道的】；同时对于一个百万像素的图像，使用全连接网络就意味着百万级别的参数，这对于计算的占用显然是不合理的，此博客主要记录基本的卷积神经网络概念和一些经典卷积神经网络模型

## 一、核心概念

### 1.1 局部感受野

CNN 的设计灵感来源于生物视觉系统。研究发现，高等动物的视觉皮层细胞对特定方向的边缘、条纹等局部特征敏感。受此启发，CNN 通过局部感受野机制，让每个神经元只关注输入图像的一个局部区域。例如，一个 3x3 的卷积核在图像上滑动，每次只处理 3x3 大小的局部图像块，这样可以大大减少参数数量，同时保留图像的局部特征。

{% include image.html url="/assets/images/dl02/image-20250622151959899.png" description="" %}

### 1.2 权值共享

权值共享是 CNN 的另一个重要特性。在传统的全连接神经网络中，每个神经元与上一层的所有神经元都有连接，这导致参数数量巨大。而在 CNN 中，卷积核的权值在整个图像上是共享的。也就是说，无论卷积核在图像的哪个位置滑动，其权值都保持不变。这样不仅减少了参数数量，降低了计算量，还提高了模型的泛化能力，使得模型对图像的平移具有不变性。

### 1.3 下采样（池化）

下采样，也称为池化操作，是 CNN 中常用的一种降维手段。其主要目的有两个：一是减少特征图的维度，从而减少后续层的参数数量和计算量，防止过拟合；二是赋予模型一定程度的平移不变性。常见的池化操作有最大池化和平均池化。最大池化是在池化窗口内取最大值作为输出，能够保留图像的主要特征；平均池化则是取平均值，对特征进行平滑处理。

{% include image.html url="/assets/images/dl02/image-20250622152052703.png" description="" %}

## 二、卷积神经网络的工作原理

### 2.1 卷积操作

卷积是 CNN 中最核心的操作。它通过一个可学习的卷积核在输入图像（或特征图）上滑动，计算卷积核与对应局部区域的点积，从而得到新的特征图。在深度学习实践中，通常实现的是互相关操作，虽然在严格数学定义中图像卷积需先翻转卷积核，但由于卷积核可学习，翻转与否不影响最终特征表示能力。卷积操作可以看作是对输入图像进行不同方向的滤波，能够提取图像中的各种特征，如边缘、纹理等。

### 2.2 填充与步长

在进行卷积操作时，为了控制输出特征图的尺寸，或者让卷积核能够处理到图像边缘的像素，常常会在输入图像的边界进行填充。通常使用 0 进行填充（zero-padding），也可以复制边界像素。步长则定义了卷积核在输入图像上滑动的步幅。步长会影响输出特征图的链接：较大的步长会导致输出尺寸减小。合理设置填充和步长，可以灵活调整特征图的大小和数量。

### 2.3 多通道卷积

当输入数据具有多个通道时（如 RGB 彩色图像有 3 个通道），卷积核也需要有相应的通道数。在 PyTorch 的 nn.Conv2d 中，in_channels 参数指定了输入通道数，out_channels 指定了输出特征图的数量（即卷积核的数量）。多通道卷积可以同时对不同通道的信息进行处理，提取更丰富的特征。

### 2.4 误差反向传播

CNN 的训练依赖于误差反向传播（Backpropagation，BP）算法来调整网络中的权重和偏置。其核心思想是将输出层的误差逐层向前传播，计算每一层参数对总误差的贡献（即梯度），然后利用梯度下降等优化算法更新参数。在 CNN 中，由于存在卷积和池化操作，BP 算法的具体形式会更复杂一些。例如，池化层的反向传播中，最大池化的误差只通过值最大的那个神经元反向传播，平均池化的误差会平均分配给池化窗口内的所有神经元；卷积层的反向传播时，误差项从下一层反向传播需要经过与前向传播卷积核 “相关” 的卷积操作（通常是原卷积核旋转 180 度），权重的梯度计算涉及输入激活与误差项的卷积 。

## 三、卷积神经网络的网络结构

一个典型的卷积神经网络通常由输入层、卷积层、激活层、池化层、全连接层和输出层堆叠而成。

1. **输入层**：接收原始数据，如图像像素。
2. **卷积层**：使用卷积核提取局部特征，是 CNN 的核心层之一。
3. **激活层**：对卷积层的输出进行非线性变换，引入非线性因素，常用的激活函数有 ReLU、Sigmoid、Tanh 等。PyTorch 中如 nn.ReLU ()。
4. **池化层**：对特征图进行下采样，减少维度。
5. **全连接层**：在网络的最后阶段，将前面提取到的特征进行整合，用于分类或回归任务。PyTorch 中如 nn.Linear ()。
6. **输出层**：输出最终结果，如分类概率。

通常，卷积层和池化层会交替出现多次，形成一个 “特征提取器”。网络的深处提取的是更抽象、更高级的特征。

## 四、经典卷积神经网络模型

### 4.1 LeNet-5

LeNet-5 是由 Yann LeCun 等人在 1998 年提出的早期卷积神经网络之一，主要用于手写数字识别，并取得了巨大成功，是 CNN 发展史上的一个里程碑。其整体结构体现了 CNN 的典型设计：卷积层、池化层交替，最后连接全连接层进行分类。LeNet-5 与现代 CNN 在填充、池化方式、激活函数、网络深度与参数量等方面存在区别 。例如，LeNet-5 在卷积时通常不进行填充，主要使用平均池化，使用 Sigmoid 或 tanh 作为激活函数，网络相对较浅，参数数量较小。

### 4.2 AlexNet

AlexNet 在 2012 年的 ImageNet 大规模视觉识别挑战赛（ILSVRC）中夺冠，它的出现掀起了深度学习的热潮。AlexNet 使用了 ReLU 激活函数，有效解决了梯度消失问题，训练速度更快。同时，它还使用了 Dropout 技术，随机丢弃一些神经元，防止过拟合。此外，AlexNet 利用 GPU 进行并行计算，大大提高了训练效率。

### 4.3 VGGNet

VGGNet 是牛津大学视觉几何组（Visual Geometry Group）提出的卷积神经网络。它的结构非常简洁，通过不断堆叠 3x3 的小卷积核和 2x2 的池化层，构建了深度为 16 - 19 层的网络。VGGNet 的优点是结构简单、易于理解和实现，其提出的 “小卷积核堆叠” 思想对后续的网络设计产生了深远影响。

## 五、卷积神经网络的应用场景

### 5.1 图像分类

图像分类是 CNN 最常见的应用之一，它的任务是将输入图像分类到预定义的类别中。例如，判断一张图片是猫还是狗，是汽车还是飞机等。在实际应用中，图像分类技术广泛应用于安防监控、医疗影像诊断、自动驾驶等领域。

### 5.2 目标检测

目标检测不仅要识别图像中物体的类别，还要确定物体的位置。CNN 通过在不同位置和尺度上滑动窗口，提取特征进行分类和定位。在智能交通系统中，目标检测可以识别道路上的车辆、行人、交通标志等；在工业检测中，可以检测产品的缺陷和质量问题。

### 5.3 语义分割

语义分割是将图像中的每个像素都进行分类，实现对图像的精细化理解。在医学图像分析中，语义分割可以将人体器官、病变区域等进行精确分割；在自动驾驶中，语义分割可以帮助车辆识别道路、行人、障碍物等。

卷积神经网络凭借其独特的结构和强大的特征提取能力，在深度学习领域取得了巨大的成功。随着技术的不断发展，CNN 在更多领域的应用将不断拓展和深化，为人们的生活带来更多的便利和创新。希望本文能帮助你对卷积神经网络有更深入的理解，激发你在这个充满潜力的领域进行探索和研究的兴趣。